{
 "metadata": {
  "name": "",
  "signature": "sha256:41753344a4fd30d98fb1e0d1c838eb9fb9ceff2bb4140739e41c26c70e038e58"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We need to check if every word in the IBC corpus is present in our common crawl vectors\n",
      "\n",
      "# So there is 5.3GB of common crawl information. We can't hope to store all of this in memory, so instead\n",
      "# we're going to get an index of word -> line, then cache it once it's accessed\n",
      "import linecache\n",
      "import numpy as np\n",
      "import codecs\n",
      "# from io import open\n",
      "\n",
      "class GloVeLoader:\n",
      "    index = {}\n",
      "    def __init__(self, filen):\n",
      "        \"\"\"Generate the word -> file position index\"\"\"\n",
      "        self.filen = filen\n",
      "        with open(self.filen, 'r', encoding='utf-8') as gf:\n",
      "            self.total_lines, self.dim = map(int,gf.readline().split())\n",
      "            self.null_vec = np.zeros(self.dim)\n",
      "            while True:\n",
      "                start_pos = gf.tell()\n",
      "                line = gf.readline()\n",
      "                if line == '':\n",
      "                    break\n",
      "                word = line.split(' ')[0]\n",
      "                self.index[word] = start_pos\n",
      "                self.total_lines += 1\n",
      "        \n",
      "    def get(self, word):\n",
      "        \"\"\"Gets the floats stored for that word using seek\"\"\"\n",
      "        #line = linecache.getline(self.filen,self.index[word])\n",
      "        # linecache does not work for this. It reads the whole thing in (virtual) memory, crashing\n",
      "        pos = self.index[word]\n",
      "        line = ''\n",
      "        with open(self.filen, 'r', encoding='utf-8') as tmp:\n",
      "            tmp.seek(pos)\n",
      "            line = tmp.readline()\n",
      "        if line == '':\n",
      "            print(\"Looked for %s (position %d), got an error\" % (word, self.index[word]))\n",
      "            # note be careful that it's returning a ref, don't mutate\n",
      "            return self.null_vec\n",
      "        vals = line.split(' ')\n",
      "        if vals[0] != word:\n",
      "            print(\"Looked for %s (position %d), got %s. Something went wrong\" % (word, self.index[word], vals[0]))\n",
      "            return self.null_vec\n",
      "        return np.array(list(map(float,vals[1:])))\n",
      "    \n",
      "    def clearcache(self):\n",
      "        \"\"\"Clear cache if the GloVe words need to be reloaded\"\"\"\n",
      "        linecache.clearcache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "tic = time.time()\n",
      "commcrawl_vecs = GloVeLoader(\"word_reps/glove/glove.840B.300d--modified.txt\")\n",
      "toc = time.time() - tic\n",
      "print(\"Took %f seconds\" % toc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Took 60.509493 seconds\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "commcrawl_vecs.index['Glass-Steagal']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "2148135976"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "IBC = pickle.load(open(\"full_ibc/ibcData.pkl\",'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictIBC = {\"liberal\":[sent.get_words().split(' ') for sent in IBC[0]],\n",
      "           \"conservative\":[sent.get_words().split(' ') for sent in IBC[1]],\n",
      "           \"neutral\":[sent.get_words().split(' ') for sent in IBC[2]]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total_problematic = 0\n",
      "for label in [\"liberal\",\"conservative\",\"neutral\"]:\n",
      "    total_sents = len(dictIBC[label])\n",
      "    sents_w_missing = 0    \n",
      "    for sent in dictIBC[label]:\n",
      "        old_p = total_problematic\n",
      "        for word in sent:\n",
      "            if word not in commcrawl_vecs.index:\n",
      "                total_problematic += 1\n",
      "        if old_p != total_problematic:\n",
      "            sents_w_missing += 1\n",
      "    print(\"%s has %d sentences with unknown words out of %d sentences total\" % (label, sents_w_missing, total_sents))\n",
      "print(total_problematic)\n",
      "skipped = set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from IPython.display import clear_output\n",
      "def insert_multi(lis, ind, ins):\n",
      "    \"\"\"Utility function to input a string of multiple tokens into a position\"\"\"\n",
      "    lis.pop(ind)\n",
      "    for ins_ind, ins_i in enumerate(ins):\n",
      "        lis.insert(ind+ins_ind,ins_i)\n",
      "    \n",
      "def get_context(lis, ind):\n",
      "    \"\"\"Utility function to get string of surrounding words, handles bounds\"\"\"\n",
      "    if ind > 0 and ind < len(lis)-1:\n",
      "        return ' '.join([lis[ind-1],lis[ind],lis[ind+1]])\n",
      "    elif ind > 0:\n",
      "        return ' '.join([lis[ind-1],lis[ind]])\n",
      "    else:\n",
      "        return ' '.join([lis[ind],lis[ind+1]])\n",
      "        \n",
      "def check_valid(lis, index):\n",
      "    in_ind = [i in index for i in lis]\n",
      "    return (all(in_ind), [lis[ind] for ind in range(len(lis)) if not in_ind[ind]])\n",
      "\n",
      "\n",
      "for label in [\"liberal\",\"conservative\",\"neutral\"]:\n",
      "    for sent in dictIBC[label]:\n",
      "        for ind, word in enumerate(sent):\n",
      "            if word not in commcrawl_vecs.index and (word not in skipped):\n",
      "                print(\"Word %s from IBC not found in Common Crawl\" % (word))\n",
      "                print(\"Context %s\" % get_context(sent, ind))\n",
      "                print(\"\"\"1: leave as is\n",
      "2: typo, let me fix\n",
      "3: split on -\n",
      "4: remove -\n",
      "5: join with left token\n",
      "6: join with right token\n",
      "7: join all three\n",
      "8: remove \\\\\"\"\")\n",
      "                command = input().rstrip()\n",
      "                if command == \"1\":\n",
      "                    skipped.add(word)\n",
      "                if command == \"2\":\n",
      "                    print(\"Ok, type it:\")\n",
      "                    new_word = input().rstrip()\n",
      "                    use = True\n",
      "                    while (True):\n",
      "                        in_ind = [i in commcrawl_vecs.index for i in new_word.split(' ')]\n",
      "                        if all(in_ind):\n",
      "                            break\n",
      "                        else:    \n",
      "                            print(\"%r not in the index. Try again? Type $$skip$$ to skip it\" % \n",
      "                                  [new_word.split(' ')[idx] for idx in range(len(in_ind)) if not in_ind[idx]])\n",
      "                        new_word = input().rstrip()\n",
      "                        if new_word == \"$$skip$$\":\n",
      "                            use=False\n",
      "                            skipped.add(word)\n",
      "                            break\n",
      "                    if use:\n",
      "                        insert_multi(sent, ind, new_word.split(' '))\n",
      "                if command == \"3\":\n",
      "                    new_words = word.split('-')\n",
      "                    insert_multi(sent, ind, new_words)\n",
      "                if command == \"4\":\n",
      "                    new_word = word.replace('-','')\n",
      "                    sent[ind] = new_word\n",
      "                if command == \"5\":\n",
      "                    if ind == 0:\n",
      "                        print(\"no. continuing\")\n",
      "                        continue\n",
      "                    new_word = sent[ind-1] + sent[ind]\n",
      "                    sent[ind] = new_word\n",
      "                    sent.pop(ind-1)\n",
      "                if command == \"6\":\n",
      "                    if ind >= len(sent):\n",
      "                        print(\"nope. continuing\")\n",
      "                        continue\n",
      "                    new_word = sent[ind] + sent[ind+1]\n",
      "                    sent[ind] = new_word\n",
      "                    sent.pop(ind+1)\n",
      "                if command == \"7\":\n",
      "                    if ind >= len(sent) or ind == 0:\n",
      "                        print(\"fuck you. continuing\")\n",
      "                        continue\n",
      "                    new_word = sent[ind-1] + sent[ind] + send[ind+1]\n",
      "                    sent[ind] = new_word\n",
      "                    sent.pop(ind+1)\n",
      "                    sent.pop(ind-1)\n",
      "                if command == \"8\":\n",
      "                    new_word = word.replace('\\\\','')\n",
      "                    sent[ind] = new_word\n",
      "                print(\"Skipping\")\n",
      "                clear_output()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(dictIBC,open(\"full_ibc/cleaned_IBCdict.pkl\",'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictIBC_cleaned = pickle.load(open(\"full_ibc/cleaned_IBCdict.pkl\",'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictIBC_cr = {key:[] for key in dictIBC_cleaned} # new dictionary to hold data that \n",
      "for label in dictIBC_cleaned:\n",
      "    for sent in dictIBC_cleaned[label]:\n",
      "        if all([word in commcrawl_vecs.index for word in sent]):\n",
      "            dictIBC_cr[label].append(sent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Remaining sentences\")\n",
      "for label in dictIBC_cr:\n",
      "    print(\"%s: %d / %d\" % (label, len(dictIBC_cr[label]), len(dictIBC_cleaned[label])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Remaining sentences\n",
        "neutral: 587 / 600\n",
        "liberal: 1984 / 2025\n",
        "conservative: 1674 / 1701\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
      "flatIBC = []\n",
      "for label in dictIBC_cr:\n",
      "    for sent in dictIBC_cr[label]:\n",
      "        flatIBC.append(' '.join(sent))\n",
      "#print(flatIBC)\n",
      "vectorizer = TfidfVectorizer(lowercase=False)\n",
      "X = vectorizer.fit_transform(flatIBC)\n",
      "idf = vectorizer._tfidf.idf_\n",
      "idfWeights = dict(zip(vectorizer.get_feature_names(), idf))\n",
      "get_tf = lambda doc: {word:doc.count(word) / float(len(doc)) for word in doc}\n",
      "# normalize IDF to maximum\n",
      "max_idf = max(idfWeights.values())\n",
      "for word in idfWeights:\n",
      "    idfWeights[word] /= max_idf\n",
      "\n",
      "def tfidf_vector(sent, loader, idf):\n",
      "    idf_safe = lambda x: idf[x] if x in idf else 1\n",
      "    tf = get_tf(sent)\n",
      "    return np.sum([loader.get(word) * (tf[word] * idf_safe(word)) for word in sent],axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.spatial.distance import cosine\n",
      "\n",
      "tfvec = tfidf_vector(\"I ran into the bushes .\".split(),commcrawl_vecs,idfWeights)\n",
      "tfvec2 = tfidf_vector(\"I walked into the bushes .\".split(),commcrawl_vecs,idfWeights)\n",
      "tfvec3 = tfidf_vector(\"Fuck her right in the pussy .\".split(),commcrawl_vecs,idfWeights)\n",
      "print(cosine(tfvec,tfvec2))\n",
      "print(cosine(tfvec,tfvec3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0373067117898\n",
        "0.397752479758\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def avg_vector(sent, loader):\n",
      "    \"\"\" Get the average vector of a sentence by just taking the sum of the vectors and dividing by sqrt(len(sent))\"\"\"\n",
      "    return np.sum([loader.get(word) for word in sent],axis=0) / np.sqrt(len(sent))\n",
      "\n",
      "vec1 = avg_vector(\"I ran into the bushes .\".split(),commcrawl_vecs)\n",
      "vec2 = avg_vector(\"I walked into the bushes .\".split(),commcrawl_vecs)\n",
      "vec3 = avg_vector(\"Fuck her right in the pussy .\".split(),commcrawl_vecs)\n",
      "print(cosine(vec1,vec2))\n",
      "print(cosine(vec1,vec3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0292427489528\n",
        "0.258901846427\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train on average*sqrt(n) vectors\n",
      "from sklearn import svm, cross_validation\n",
      "X = []\n",
      "y = []\n",
      "for label in dictIBC_cr:\n",
      "    for sent in dictIBC_cr[label]:\n",
      "        X.append(avg_vector(sent,commcrawl_vecs))\n",
      "        y.append(label)\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=0.25)\n",
      "linear_svc = svm.SVC().fit(X_train, y_train)\n",
      "print(linear_svc.score(X_test,y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}