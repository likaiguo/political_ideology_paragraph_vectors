{
 "metadata": {
  "name": "",
  "signature": "sha256:063fcc6cca2a5f4fa612eae181be4b0f2410c855257aa5cbc0a2c9a38bd46861"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We need to check if every word in the IBC corpus is present in our common crawl vectors\n",
      "\n",
      "# So there is 5.3GB of common crawl information. We can't hope to store all of this in memory, so instead\n",
      "# we're going to get an index of word -> line, then cache it once it's accessed\n",
      "import linecache\n",
      "import numpy as np\n",
      "import codecs\n",
      "# from io import open\n",
      "\n",
      "class GloVeLoader:\n",
      "    index = {}\n",
      "    def __init__(self, filen):\n",
      "        \"\"\"Generate the word -> file position index\"\"\"\n",
      "        self.filen = filen\n",
      "        with open(self.filen, 'r') as gf:\n",
      "            self.total_lines, self.dim = map(int,gf.readline().split())\n",
      "            self.null_vec = np.zeros(self.dim)\n",
      "            while True:\n",
      "                start_pos = gf.tell()\n",
      "                line = gf.readline()\n",
      "                if line == '':\n",
      "                    break\n",
      "                word = line.split(' ')[0]\n",
      "                self.index[word] = start_pos\n",
      "                self.total_lines += 1\n",
      "        \n",
      "    def get(self, word):\n",
      "        \"\"\"Gets the floats stored for that word using seek\"\"\"\n",
      "        #line = linecache.getline(self.filen,self.index[word])\n",
      "        # linecache does not work for this. It reads the whole thing in (virtual) memory, crashing\n",
      "        pos = self.index[word]\n",
      "        line = ''\n",
      "        with open(self.filen, 'r', encoding='utf-8') as tmp:\n",
      "            tmp.seek(pos)\n",
      "            line = tmp.readline()\n",
      "        if line == '':\n",
      "            print(\"Looked for %s (position %d), got an error\" % (word, self.index[word]))\n",
      "            # note be careful that it's returning a ref, don't mutate\n",
      "            return self.null_vec\n",
      "        vals = line.split(' ')\n",
      "        if vals[0] != word:\n",
      "            print(\"Looked for %s (position %d), got %s. Something went wrong\" % (word, self.index[word], vals[0]))\n",
      "            return self.null_vec\n",
      "        return np.array(list(map(float,vals[1:])))\n",
      "    \n",
      "    def clearcache(self):\n",
      "        \"\"\"Clear cache if the GloVe words need to be reloaded\"\"\"\n",
      "        linecache.clearcache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "tic = time.time()\n",
      "commcrawl_vecs = GloVeLoader(\"word_reps/glove/glove.840B.300d--modified.txt\")\n",
      "toc = time.time() - tic\n",
      "print(\"Took %f seconds\" % toc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Took 152.188187 seconds\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "commcrawl_vecs.index['Glass-Steagal']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "2148135976"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "IBC = pickle.load(open(\"full_ibc/ibcData.pkl\",'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictIBC = {\"liberal\":[sent.get_words().split(' ') for sent in IBC[0]],\n",
      "           \"conservative\":[sent.get_words().split(' ') for sent in IBC[1]],\n",
      "           \"neutral\":[sent.get_words().split(' ') for sent in IBC[2]]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total_problematic = 0\n",
      "for label in [\"liberal\",\"conservative\",\"neutral\"]:\n",
      "    total_sents = len(dictIBC[label])\n",
      "    sents_w_missing = 0    \n",
      "    for sent in dictIBC[label]:\n",
      "        old_p = total_problematic\n",
      "        for word in sent:\n",
      "            if word not in commcrawl_vecs.index:\n",
      "                total_problematic += 1\n",
      "        if old_p != total_problematic:\n",
      "            sents_w_missing += 1\n",
      "    print(\"%s has %d sentences with unknown words out of %d sentences total\" % (label, sents_w_missing, total_sents))\n",
      "print(total_problematic)\n",
      "skipped = set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from IPython.display import clear_output\n",
      "def insert_multi(lis, ind, ins):\n",
      "    \"\"\"Utility function to input a string of multiple tokens into a position\"\"\"\n",
      "    lis.pop(ind)\n",
      "    for ins_ind, ins_i in enumerate(ins):\n",
      "        lis.insert(ind+ins_ind,ins_i)\n",
      "    \n",
      "def get_context(lis, ind):\n",
      "    \"\"\"Utility function to get string of surrounding words, handles bounds\"\"\"\n",
      "    if ind > 0 and ind < len(lis)-1:\n",
      "        return ' '.join([lis[ind-1],lis[ind],lis[ind+1]])\n",
      "    elif ind > 0:\n",
      "        return ' '.join([lis[ind-1],lis[ind]])\n",
      "    else:\n",
      "        return ' '.join([lis[ind],lis[ind+1]])\n",
      "        \n",
      "def check_valid(lis, index):\n",
      "    in_ind = [i in index for i in lis]\n",
      "    return (all(in_ind), [lis[ind] for ind in range(len(lis)) if not in_ind[ind]])\n",
      "\n",
      "\n",
      "for label in [\"liberal\",\"conservative\",\"neutral\"]:\n",
      "    for sent in dictIBC[label]:\n",
      "        for ind, word in enumerate(sent):\n",
      "            if word not in commcrawl_vecs.index and (word not in skipped):\n",
      "                print(\"Word %s from IBC not found in Common Crawl\" % (word))\n",
      "                print(\"Context %s\" % get_context(sent, ind))\n",
      "                print(\"\"\"1: leave as is\n",
      "2: typo, let me fix\n",
      "3: split on -\n",
      "4: remove -\n",
      "5: join with left token\n",
      "6: join with right token\n",
      "7: join all three\n",
      "8: remove \\\\\"\"\")\n",
      "                command = input().rstrip()\n",
      "                if command == \"1\":\n",
      "                    skipped.add(word)\n",
      "                if command == \"2\":\n",
      "                    print(\"Ok, type it:\")\n",
      "                    new_word = input().rstrip()\n",
      "                    use = True\n",
      "                    while (True):\n",
      "                        in_ind = [i in commcrawl_vecs.index for i in new_word.split(' ')]\n",
      "                        if all(in_ind):\n",
      "                            break\n",
      "                        else:    \n",
      "                            print(\"%r not in the index. Try again? Type $$skip$$ to skip it\" % \n",
      "                                  [new_word.split(' ')[idx] for idx in range(len(in_ind)) if not in_ind[idx]])\n",
      "                        new_word = input().rstrip()\n",
      "                        if new_word == \"$$skip$$\":\n",
      "                            use=False\n",
      "                            skipped.add(word)\n",
      "                            break\n",
      "                    if use:\n",
      "                        insert_multi(sent, ind, new_word.split(' '))\n",
      "                if command == \"3\":\n",
      "                    new_words = word.split('-')\n",
      "                    insert_multi(sent, ind, new_words)\n",
      "                if command == \"4\":\n",
      "                    new_word = word.replace('-','')\n",
      "                    sent[ind] = new_word\n",
      "                if command == \"5\":\n",
      "                    if ind == 0:\n",
      "                        print(\"no. continuing\")\n",
      "                        continue\n",
      "                    new_word = sent[ind-1] + sent[ind]\n",
      "                    sent[ind] = new_word\n",
      "                    sent.pop(ind-1)\n",
      "                if command == \"6\":\n",
      "                    if ind >= len(sent):\n",
      "                        print(\"nope. continuing\")\n",
      "                        continue\n",
      "                    new_word = sent[ind] + sent[ind+1]\n",
      "                    sent[ind] = new_word\n",
      "                    sent.pop(ind+1)\n",
      "                if command == \"7\":\n",
      "                    if ind >= len(sent) or ind == 0:\n",
      "                        print(\"fuck you. continuing\")\n",
      "                        continue\n",
      "                    new_word = sent[ind-1] + sent[ind] + send[ind+1]\n",
      "                    sent[ind] = new_word\n",
      "                    sent.pop(ind+1)\n",
      "                    sent.pop(ind-1)\n",
      "                if command == \"8\":\n",
      "                    new_word = word.replace('\\\\','')\n",
      "                    sent[ind] = new_word\n",
      "                print(\"Skipping\")\n",
      "                clear_output()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(dictIBC,open(\"full_ibc/cleaned_IBCdict.pkl\",'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictIBC_cleaned = pickle.load(open(\"full_ibc/cleaned_IBCdict.pkl\",'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "unsupported pickle protocol: 3",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-546e8e8167ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictIBC_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"full_ibc/cleaned_IBCdict.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_proto\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[0mproto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mproto\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"unsupported pickle protocol: %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPROTO\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: unsupported pickle protocol: 3"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictIBC_cr = {key:[] for key in dictIBC_cleaned} # new dictionary to hold data that \n",
      "for label in dictIBC_cleaned:\n",
      "    for sent in dictIBC_cleaned[label]:\n",
      "        if all([word in commcrawl_vecs.index for word in sent]):\n",
      "            dictIBC_cr[label].append(sent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Remaining sentences\")\n",
      "for label in dictIBC_cr:\n",
      "    print(\"%s: %d / %d\" % (label, len(dictIBC_cr[label]), len(dictIBC_cleaned[label])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
      "flatIBC = [item for sublist in dictIBC_cr for item in sublist]\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(flatIBC)\n",
      "idf = vectorizer._tfidf.idf_\n",
      "#MAY NEED TO DEBUG THIS CONSTRUCTION OF THE DICT.\n",
      "tfidfWeights = dict(zip(vectorizer.get_feature_names(), idf))   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def avg_vector(sent, loader):\n",
      "    \"\"\" Get the average vector of a sentence by just taking the sum of the vectors and dividing by sqrt(len(sent))\"\"\"\n",
      "    return np.sum([loader.get(word)*tfidfWeights[word] for word in sent],axis=0) / np.sqrt(len(sent))\n",
      "\n",
      "vec1 = avg_vector(\"I ran into the bushes .\".split(),commcrawl_vecs)\n",
      "vec2 = avg_vector(\"I walked into the bushes .\".split(),commcrawl_vecs)\n",
      "vec3 = avg_vector(\"Fuck her right in the pussy .\".split(),commcrawl_vecs)\n",
      "from scipy.spatial.distance import cosine\n",
      "print(cosine(vec1,vec2))\n",
      "print(cosine(vec1,vec3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm, cross_validation\n",
      "X = []\n",
      "y = []\n",
      "for label in dictIBC_cr:\n",
      "    for sent in dictIBC_cr[label]:\n",
      "        X.append(avg_vector(sent,commcrawl_vecs))\n",
      "        y.append(label)\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=0.25)\n",
      "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7).fit(X_train, y_train)\n",
      "print(rbf_svc.score(X_test,y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}